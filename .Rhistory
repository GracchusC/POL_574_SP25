"list_distinct_implmnt_years_bnd", "initial_negot_rights_bnd", "hscode_desc_bnd")
names(bound) <- colnms
bound <- readxl::read_xls("/Users/christianbaehr/Downloads/DataExport_18_6_2024__14_48_38.xls", sheet = "Bound")
colnms <- c("reporter", "hscode_lvl", "hscode_version", "hscode", "num_subhdngs_bnd", "binding_status_bnd", "num_tl_bnd",
"num_avduties_bnd", "avg_avduties_bnd", "min_avduty_bnd", "max_avduty_bnd", "dutyfree_tl_bnd_pct", "num_nonav_duty",
"list_non_avduties_hs6only_bnd", "num_tl_w_ssg_bnd", "num_tl_w_dutiable_odc_bnd", "avg_odc_av_duties_bnd", "num_odc_nonav_duties_bnd", "list_odc_non_avduties_hs6only_bnd", "list_distinct_present_instruments_bnd",
"list_distinct_implmnt_years_bnd", "initial_negot_rights_bnd", "hscode_desc")
names(bound) <- colnms
bound <- bound[-c(1:4) , ]
bound <- bound[which(bound$hscode_lvl == "2") , ]
View(bound)
dat <- read.csv("/Users/christianbaehr/Downloads/WtoData_20240618214437.csv", stringsAsFactors = F)
View(dat)
sort(unique(dat$Indicator.Category))
sort(unique(dat$Indicator.Code))
dat <- read.csv("/Users/christianbaehr/Downloads/WtoData_20240618215753.csv", stringsAsFactors = F)
View(dat[1:2000 , ])
table(dat$Partner.Economy.Code)
table(dat$Indicator)
table(dat$Indicator.Code)
table(dat$Year)
dat$Value[which(dat$Year==2010 & dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020")]
View(dat$Value[which(dat$Year==2010 & dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020")])
View(dat[which(dat$Year==2010 & dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020") , ])
which(dat$Year==2010 & dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020" & dat$Product.Sector.Code=="01")
View(dat[ind , ])
ind <- which(dat$Year==2010 & dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020" & dat$Product.Sector.Code=="01")
View(dat[ind , ])
ind <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator.Code=="HS_A_0020" & dat$Product.Sector.Code=="01")
View(dat[ind , ])
sort(unique(dat$Indicator))
ind <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="Bnd  -  Maximum duty")
View(dat[ind , ])
ind <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="Bnd  -  Maximum duty by product groups")
View(dat[ind , ])
ind <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="Bnd AG -  Simple average duty")
View(dat[ind , ])
ind <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="HS Bnd - Simple average ad valorem duty")
View(dat[ind , ])
ind1 <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="HS Bnd - Simple average ad valorem duty")
View(dat[ind1 , ])
"HS MFN - Simple average ad valorem duty"
ind2 <- which(dat$Reporting.Economy.ISO3A.Code=="BRA" & dat$Indicator=="HS MFN - Simple average ad valorem duty"  )
View(dat[ind2 , ])
rm(list = ls())
load("/Users/christianbaehr/Downloads/full_upsampled_10.Rdata")
View(full)
write.csv(full, "/Users/christianbaehr/Downloads/full_upsampled_10.csv", row.names = F)
temp <- read.csv("/Users/christianbaehr/Downloads/WtoData_20240806174712.csv", stringsAsFactors = F)
View(temp)
table(nchar(temp$Product.Sector.Code))
temp2 <- temp[which(temp$Reporting.Economy.ISO3A.Code=="USA") , ]
table(temp$Reporting.Economy.ISO3A.Code)
temp$Product.Sector.Code <- ifelse(nchar(temp$Product.Sector.Code)==6,
temp$Product.Sector.Code,
paste0("0", temp$Product.Sector.Code))
sort(unique(temp$Product.Sector.Code))
sort(unique(temp$Product.Sector.Code[substr(temp$Product.Sector.Code,1,1)=="6"]))
temp2 <- temp[which(temp$Product.Sector.Code=="620640"),]
View(temp2)
#temp <- read.csv("/Users/christianbaehr/Downloads/WtoData_20240806174712.csv", stringsAsFactors = F)
temp <- read.csv("/Users/christianbaehr/Downloads/WtoData_20240806184602.csv", stringsAsFactors = F)
load("/Users/christianbaehr/Library/CloudStorage/Dropbox/UN_speech_data/Data/Turco_UNGA_speech/UNGA Speech Corpus Data/unga_speech_corpus.RData")
View(txtstatesyears)
View(txtstatesyears)
txtstatesyears$text[1]
rm(txtstatesyears)
list.file("/Users/christianbaehr/Dropbox/Flood_Insurance/Data/Input/hazards_unzipped/NFHL_12_20141203.gdb/")
list.files("/Users/christianbaehr/Dropbox/Flood_Insurance/Data/Input/hazards_unzipped/NFHL_12_20141203.gdb/")
sort(list.files("/Users/christianbaehr/Dropbox/Flood_Insurance/Data/Input/hazards_unzipped/NFHL_12_20141203.gdb/"))
dat <- readxl::read_xls("/Users/christianbaehr/Downloads/2014WWFieldProd - yearly production only.xls")
View(dat)
df <- read.csv("/Users/christianbaehr/Downloads/Pennsylvania_Local_Roads.csv", stringsAsFactors = F)
table(df$STRUCTURAL_CONDITION)
sum(!is.na(df$STRUCTURAL_CONDITION))/nrow(df)
View(df)
install.packages("modelsummary")
remove.packages("tinytable")
remove.packages("insight")
remove.packages("performance")
remove.packages("parameters")
devtools::install_github("xuyiqing/fect")
install.packages("fect")
install.packages("devtools")
devtools::install_github("xuyiqing/fect")
R.Version()
devtools::install_github("xuyiqing/fect")
install.packages("devtools")
devtools::install_github("xuyiqing/fect")
install.packages("shiny")
run .libPaths()
.libPaths()
devtools::install_github("rstudio/httpuv", lib = "/Library/Frameworks/R.framework/Versions/4.2/Resources/library")
devtools::install_github("rstudio/shiny", lib = "/Library/Frameworks/R.framework/Versions/4.2/Resources/library")
install.packages("pacman")
pacman::p_load(httpuv)
R.Version()
install.packages("devtools")
install.packages("fect")
help(fect)
library(fect)
help(fect)
help(remove.packages)
remove.packages("fect")
devtools::install_github("xuyiqing/fect")
library(devtools)
help(install_github)
devtools::install_github("xuyiqing/fect", upgrade="always")
devtools::install_github("xuyiqing/fect", upgrade="always")
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2,
readtext, stringr, gutenbergr, stylest2, text.alignment)
setwd("/Users/christianbaehr/Dropbox/POL 574/Homework/")
load("hw1/unga_speech_corpus.RData")
doc1 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="United States of America")[1]
doc2 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="United Kingdom")[1]
doc3 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="Australia")[1]
txtstatesyears <- txtstatesyears[c(doc1, doc2, doc3) , ]
txtstatesyears$country[which(txtstatesyears$country=="United States of America")] <- "US"
txtstatesyears$country[which(txtstatesyears$country=="United Kingdom")] <- "UK"
## convert to corpus, with "text" as the variable with text data
unga.corpus <- corpus(txtstatesyears, text_field = "text", docid_field = "country")
## tokenize the speeches (no pre processing yet)
unga.tokens <- tokens(unga.corpus)
## function to compute TTR
##
## @param x tokenized quanteda corpus
calculate_TTR <- function(x){
ntype(x)/lengths(x)
}
calculate_TTR(unga.tokens)
## function to compute Guiraud's index of lexical richness
##
## @param x tokenized quanteda corpus
calculate_G <- function(x){
ntype(x)/sqrt(lengths(x))
}
calculate_G(unga.tokens)
## create dfm of the UNGA speeches
unga.dfm <- tokens(unga.corpus, remove_punct = T) |>
dfm(tolower=F)
textstat_simil(unga.dfm, margin = "documents", method = "cosine")
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = TRUE) |>
tokens_wordstem()
## TTR
ttr <- calculate_TTR(unga.tokens) %>% setNames(c("USA", "UK", "France"))
r <- calculate_G(unga.tokens) %>% setNames(c("USA", "UK", "France"))
## Similarity
unga.dfm <- dfm(unga.tokens, tolower = FALSE)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")
## print results
cat("TTR scores w. stemming: \n", ttr, "\n\n")
cat("G scores w. stemming: \n", r, "\n\n")
cat("Cosine similarity w. stemming: \n"); prmatrix(as.matrix(sim))
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = TRUE) |>
tokens_remove(stopwords("english"))
## TTR
ttr <- calculate_TTR(unga.tokens)
r <- calculate_G(unga.tokens) %>% setNames(c("US", "UK", "France"))
## Similarity
unga.dfm <- dfm(unga.tokens, tolower = FALSE)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")
## print results
cat("TTR scores w/o stopwords: \n", ttr, "\n\n")
cat("G scores w/o stopwords: \n", r, "\n\n")
cat("Cosine similarity w/o stopwords: \n"); prmatrix(as.matrix(sim))
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = TRUE) |>
tokens_tolower()
## TTR
ttr <- calculate_TTR(unga.tokens)
r <- calculate_G(unga.tokens)
## Similarity
unga.dfm <- dfm(unga.tokens)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")
# print results
cat("TTR scores w. lowercase: \n", ttr, "\n\n")
cat("G scores w. lowercase: \n", r, "\n\n")
cat("Cosine similarity w. lowercases: \n"); prmatrix(as.matrix(sim))
unga.dfm.tfidf <- tokens(unga.corpus, remove_punct = T) %>%
dfm() %>%
dfm_tfidf()
textstat_simil(unga.dfm.tfidf, margin = "documents", method = "cosine")
## file names
files <- c("hw1/wealth_of_nations.txt", "hw1/theory_moral_sentiments.txt")
## read each text as a corpus object
smith <- readtext(files) %>%
corpus()
## docvar with titles
smith$title <- c("wealth of nations", "theory of moral sentiments")
## lowercase and remove hyphens
smith <- tolower(smith)
smith <- gsub("-", " ", smith)
## remove symbols/punctuation/numbers/stopwords
smith.tok <- tokens(smith, remove_symbols = T, remove_punct = T, remove_numbers = T) |>
tokens_remove(stopwords())
## use tfidf weighting with numerator the proportion of
## document tokens of that type
smith.dfm <- dfm(smith.tok) |>
dfm_tfidf(scheme_tf = "prop", base = exp(1))
topfeatures(smith.dfm[1,]) # pretty close!
sentence1 <- "Trump's immigration crackdown sparks humanitarian crisis at the border"
sentence2 <- "Trump's immigration reforms strengthen national security and economy"
## remove punctuation and tokenize
sentences.tokens <- corpus(c(sentence1, sentence2)) |>
tokens(remove_punct = T)
sentences.dfm <- dfm(sentences.tokens, tolower = T)
s1 <- as.matrix(sentences.dfm)[1,] # feature vector for sentence1
s2 <- as.matrix(sentences.dfm)[2,] # feature vector for sentence2
## Euclidean distance
euclidean <- sqrt( sum( ( s1-s2 )^2 ) )
## Manhattan distance
manhattan <- sum( abs( s1-s2 ) )
## Jaccard distance
num <- length( intersect(sentences.tokens[[1]], sentences.tokens[[2]]) )
denom <- length( union(sentences.tokens[[1]],sentences.tokens[[2]]) )
jaccard <- num / denom
## Cosine similarity
cosine <- sum(s1 * s2) /( sqrt(sum(s1^2)) * sqrt(sum(s2^2)) )
## Levenshtein distance for surveyance and surveillance
levenshtein <- adist("surveyance", "surveillance")
## print
cat("Euclidean distance:", euclidean, "\n\n",
"Manhattan distance:", manhattan, "\n\n",
"Jaccard similarity:", jaccard, "\n\n",
"Cosine similarity:", cosine, "\n\n",
"Levenshtein distance:", levenshtein)
## get text from UK political manifestos speeches
corpus <- corpus_subset(data_corpus_ukmanifestos, Year %in% c(1945:1955))
text <- tokens(corpus, remove_punct = T) |>
paste(collapse = " ") |>
tolower()
## get entry of contingency table for the collocation
o11 <- str_count(text, "northern(?= ireland)")
o12 <- str_count(text, "northern(?! ireland)")
o21 <- str_count(text, "(?<!northern )ireland")
N <- tokens(text) |>
tokens_ngrams(n = 2) |>
ntoken() |>
unname()
o22 <- N - o21 - o11 - o12
## contingency table
out <- matrix(c(o11, o12, o21, o22),
ncol = 2,
byrow = T)
rownames(out) <- c("Northern", "Not Northern")
colnames(out) <- c("Ireland", "Not Ireland")
print(out)
## expected frequency
E11 <- (o11+o12)/N * (o11 + o21)/N * N
# N12 <- N - (o11 + o21)
# E21 <- (o11+o21)/N * N21/N * N
# E12 <- (o11+o12)/N * N12/N * N
# E22 <- N12/N * N21/N * N
## get Chi-square value
## (o11-E11)^2/E11 + (o21-E21)^2/E21 + (o12-E12)^2/E12 + (o22-E22)^2/E22
## print
cat("Observed frequency:", o11, "\n\n",
"Expected frequency:", E11)
textstat_collocations(corpus, min_count = 5) %>%
data.frame() %>%
select(c("collocation", "lambda", "z")) %>%
filter(collocation == "united kingdom")
(collout1 <- textstat_collocations(corpus, min_count = 5) |>
arrange(-lambda) |>
slice(1:10) |>
data.frame() |>
select(c("collocation", "count", "lambda", "z")))
(collout2 <- textstat_collocations(corpus, min_count = 5) |>
arrange(-count) |>
slice(1:10) |>
data.frame() |>
select(c("collocation", "count", "lambda", "z")))
# ## Prepare data function
# ##
# ## @param book_id: book_id as it would appear in gutenbergr
# ## @param removePunct logical specifying whether to remove punctuation
# prepare_dt <- function(book_id, removePunct = TRUE){
#   meta <- gutenberg_works(gutenberg_id  == book_id)
#   meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
#   text <- gutenberg_download(book_id) %>%
#                     select(text) %>%
#                     filter(text!="") %>%
#                     unlist() %>%
#                     paste(., collapse = " ") %>%
#                     str_squish(.) %>%
#                     str_trim(., side = "both")
#   text <- gsub("`|'", "", text) # remove apostrophes
#   text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
#   output <- tibble(title = meta$title, author = meta$author, text = text)
# }
#
# ## run function
# novels <- lapply(c(64317, 2489), prepare_dt, removePunct = TRUE) %>% do.call(rbind,.)
#
# ## create dfm
# dfm <- tokens(novels$text, remove_punct = T) |>
#   dfm(tolower = T)
dfm <- smith.dfm[ 1, ]
## regression to check if slope is approx -1.0
regression <- lm(log(topfeatures(dfm, 100)) ~ log(1:100))
summary(regression)
confint(regression)
# create plot to illustrate zipf's law
plot(log(1:100), log(topfeatures(dfm, 100)),
xlab="log(rank)", ylab="log(frequency)", main="Top 100 Words")
abline(regression, col="red")
abline(a = regression$coefficients["(Intercept)"], b = -1, col = "black")
## Heap's Law
## M = kT^b
## where:
## M = vocab size
## T = number of tokens
## k, b are constants
num_tokens <- sum(rowSums(dfm))
M <- nfeat(dfm)
k <- 44
## solve for b
b <- log(M/k)/log(num_tokens)
print(b)
sotu.sub <- data_corpus_sotu
sotu.sub$Date <- format(sotu.sub$Date, "%Y")
names(docvars(sotu.sub))[3] <- "year"
sotu.sub <- sotu.sub |>
corpus_subset(year %in% 1982:2020) |>
corpus_reshape("sentence")
sotu.df <- cbind(as.character(sotu.sub), docvars(sotu.sub)["year"]) |>
setNames(c("text", "year"))
sotu.split <- split(sotu.df, as.factor(sotu.df$year))
unga.df.sub <- txtstatesyears %>%
filter(country=="United States of America" & year %in% c(2005:2015))
unga.sub <- corpus(unga.df.sub, text_field="text")
docvars(unga.sub, "year") <- unga.df.sub$year
unga.sub <- unga.sub |>
corpus_reshape("sentence")
unga.df <- cbind(as.character(unga.sub), docvars(unga.sub)["year"]) |>
setNames(c("text", "year"))
unga.split <- split(unga.df, as.factor(unga.df$year))
boot.fre <- function(year) { # accepts df of texts (year-specific)
n <- nrow(year) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- corpus(year[docnums, "text"])
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
lapply(unga.split, boot.fre) # apply to each df of party texts
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(unga.split, boot.fre)
print(paste("Iteration", i))
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sort(unique(unga.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
#coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
## mean Flesch statistic by year
flesch_point <- unga.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(unga.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
## calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(unga.sub, c("Flesch", "FOG"))
## compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
## print
print(readability_cor[1,2])
docs <- corpus( readtext(c("hw1/jefferson_draft.txt", "hw1/final_version.txt")))
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase match, decrease mismatch --> increases extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words",
match= 3,
mismatch = 0,
gap=-1)
print(sw2$sw)
print(sw3$sw)
rc <- read.csv("hw1/countypres_2000-2020.csv")
rc20 <- rc[rc$year=="2020",]
rc20DR <- rc20[rc20$party%in%c("REPUBLICAN", "DEMOCRAT"),]
# look at specific state
# TX works
state <- "VT"
rc_state <- rc20DR[rc20DR$state_po==state,]
#look at leading digit v frequency
digits <- rc_state$candidatevotes
first_digits <-  as.numeric(substr(digits, 1, 1))
rc_state$first_digs <- first_digits
dems <- rc_state[rc_state$party=="DEMOCRAT",]
total_dems <- table(factor(dems$first_digs, levels=1:9) )
rep <-  rc_state[rc_state$party=="REPUBLICAN",]
total_reps <- table(factor(rep$first_digs, levels=1:9) )
#benford expectations
ben_props  <- c(0.301, .176, .125, .097, 0.079, 0.067, 0.058, 0.051, 0.046)
names(ben_props)<- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
the_benprops <- ben_props*length(unique(rc_state$county_name))
# now do barplot
dat <- rbind(total_dems, the_benprops, total_reps)
rownames(dat) <- c("dem", "benford", "rep")
colnames(dat) <- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
#x11()
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
# article:
# https://www.reuters.com/article/world/fact-check-deviation-from-benfords-law-does-not-prove-election-fraud-idUSKBN27Q3A9/
# Election Integrity Partnership
# look at magnitude of districts
x11()
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
state <- "TX"
rc_state <- rc20DR[rc20DR$state_po==state,]
#look at leading digit v frequency
digits <- rc_state$candidatevotes
first_digits <-  as.numeric(substr(digits, 1, 1))
rc_state$first_digs <- first_digits
dems <- rc_state[rc_state$party=="DEMOCRAT",]
total_dems <- table(factor(dems$first_digs, levels=1:9) )
rep <-  rc_state[rc_state$party=="REPUBLICAN",]
total_reps <- table(factor(rep$first_digs, levels=1:9) )
#benford expectations
ben_props  <- c(0.301, .176, .125, .097, 0.079, 0.067, 0.058, 0.051, 0.046)
names(ben_props)<- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
the_benprops <- ben_props*length(unique(rc_state$county_name))
# now do barplot
dat <- rbind(total_dems, the_benprops, total_reps)
rownames(dat) <- c("dem", "benford", "rep")
colnames(dat) <- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
#x11()
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
# install.packages("devtools")
devtools::install_github("oscarkjell/text")
dyn.load(jli, FALSE)
install.packages("rJava", repos="https://rforge.net")
install.packages("rJava")
# install.packages("devtools")
devtools::install_github("oscarkjell/text")
library(rJava)
install.packages("rJava")
library("rJava")
load("/Users/christianbaehr/Dropbox/Protected_Areas/Data/Input/Sanford_Replication/data/output/full.Rdata")
load("/Users/christianbaehr/Dropbox/Protected_Areas/Data/Input/Sanford_Replication/data/output/full.Rdata")
help(smith_waterman)
help(text.alignment::smith_waterman)
help(smith_waterman)
help(textstat_collocations)
help(textstat_collocation)
help(textstats_collocation)
help(textstat_collocations)
library(quanteda.textstats)
help(textstat_collocations)
help(stringdist)
library(stringdist)
help(stringdist)
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## load packages
pacman::p_load(quanteda, quanteda.corpora, readtext, quanteda.textmodels,
quanteda.textplots, dplyr)
## 1) Naive Bayes --------------------------------------------------------------
## source of data: https://www.kaggle.com/rmisra/news-category-dataset#News_Category_Dataset_v2.json
## load in the news headline data
news_data <- readRDS("data/news_data.rds")
news_data <- news_data[, c("category", "he
news_data <- news_data[, c("category", "headline")] # keep relevant columns
news_data <- news_data[, c("category", "headline")] # keep relevant columns
news_data$headline <- gsub("'", "", news_data$headline) # remove apos
## create a corpus with just Sports and Arts headlines
news_corpus <- news_data |>
corpus(text_field = "headline") |>
corpus_subset(category %in% c("SPORTS", "ARTS"))
news_dfm <- news_data$headline |>
tokens() |>
dfm()
news_dfm <- news_data$headline |>
tokens() |>
dfm()
help(corpus)

reviews.pos[1]
reviews.pos.dfm <- dfm(reviews.pos.tokens)
reviews.pos.dfm[1,]
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en")) |>
tokens_remove("br")
tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleantokens
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleandfm <- dfm(reviews.pos.cleantokens)
dim(reviews.pos.dfm)
dim(reviews.pos.cleandfm) # fewer features
(2840-2633) / 2840 # pre processing dropped ~7% of features from unprocessed
## pull the top n frequently appearing features from the document feature matrix
topfeatures(reviews.pos.cleandfm, n=20)
## wordcloud visual of top features
library(quanteda.textplots)
textplot_wordcloud(reviews.pos.cleandfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en")) |>
tokens_remove("br")
reviews.pos.cleandfm <- dfm(reviews.pos.cleantokens)
dim(reviews.pos.dfm)
dim(reviews.pos.cleandfm) # fewer features
(2840-2633) / 2840 # pre processing dropped ~7% of features from unprocessed
## 4.4) we can sanity check our document feature matrix
## pull the top n frequently appearing features from the document feature matrix
topfeatures(reviews.pos.cleandfm, n=20)
## wordcloud visual of top features
library(quanteda.textplots)
textplot_wordcloud(reviews.pos.cleandfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/") # change the working directory
setwd("../") # move one directory back
ls() # objects in the global environment
mysentence <- "If we played 'em ten times, they might win nine. But not this game. Not...tonight."
tolower(mysentence) # lowercase
toupper(mysentence) # uppercase
gsub("not", "NOT", mysentence) # substitute "not" with "NOT"
gsub(",", "", mysentence) # works for punctuation
gsub("not", "NOT", mysentence) # substitute "not" with "NOT"
gsub(",", "", mysentence) # works for punctuation
gsub(".", "", mysentence) # some characters are "special"
gsub("\\.", "", mysentence) # escape with double backslash
gsub(".", "", mysentence) # some characters are "special"
remove.periods <- function(text) {
out <- gsub("\\.", " ", text) # note the escape character "\\" for period
return(out) # return() to retrieve output
}
remove.periods(mysentence)
mycleanwords <- strsplit(mycleansentence, split = " ")[[1]] # strsplit() breaks on split
## identify which elements contain a specified string
grep("not", mycleanwords) # indices that CONTAIN not
grep("play", mycleanwords) # indices that CONTAIN play
grep("play", mycleanwords, value = T) # return entire matching word/s
grepl("play", mycleanwords) # boolean
grep("not", mycleanwords)
grep("play", mycleanwords)
grep("play", mycleanwords, value = T)
grepl("play", mycleanwords)
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
summary(reviews, n=ndoc(reviews))
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
corpus_reshape(reviews, to = "sentences")
tokens_wordstem()
stopwords("en")
"br"
reviews
reviews[1]
reviews[[1]]
help("stopwords") # quanteda has stopwords for MANY languages
help("quanteda::stopwords") # quanteda has stopwords for MANY languages
stopwords("aq")
stopwords("sq")
stopwords("da")
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
help(dfm_tfidf)
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # dfm() defaults to "absolute frequency"
help(dfm_tfidf)
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
help(kwic)
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
## how to interpret?
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc.3
## Introduction to Text-as-Data in R
## Date: February 6, 2025
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## Topics:
## - dfm and tf idf
## - examining your corpus
## - regular expressions
## - preText
################################################# Precept 2: Processing Text in R
## 1.1) Working Directory ------------------------------------------------------
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## 1.2) Package Management -----------------------------------------------------
## only do these once
#install.packages("pacman")
#install.packages("devtools")
#devtools::install_github("matthewjdenny/preText")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## 1.3) Load Movie Reviews Into a Corpus ---------------------------------------
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## what constitutes a DOCUMENT in this corpus?
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove( stopwords("en") ) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
## weighted dfms
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
#######################################
## 1.4) IN CLASS ACTIVITY
## Working in pairs, compute the difference-in-means for the RATE AT WHICH
## the term "bad" occurs in positive reviews versus negative reviews
## Hint: retrieve non-zero entries for a single dfm feature column
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
#######################################
## 2.1) Key Word in Context ----------------------------------------------------
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
## 2.2) Collocations -----------------------------------------------------------
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
## how to interpret?
## trigrams
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc.3
## all n-grams between two and four
reviews.colloc.2_4 <- textstat_collocations(reviews, size = 2:4)
reviews.colloc.2_4
## break this sentence a character vector of words
mysentence <- "We've never lost an American in space, we're sure as hell not gonna lose one on my watch! Failure is not an option."
mysentence <- strsplit(mysentence, split = " ")[[1]]
grep("a", mysentence, value = T) # any element with "a"
grep("^a", mysentence, value = T) # any element STARTING with "a"
grep("[[:punct:]]", mysentence, value = T) # any element with punctuation
grep("^F+[a-z0-9]+ure$", mysentence, value = T)
grep("^F{1}[a-z0-9]{3}ure$", mysentence, value = T) # why does this work?
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
head(preprocessed_documents$choices)
preText_results <- preText(preprocessed_documents,
dataset_name = "SOTU Speeches",
distance_method = "cosine",
num_comparisons = 20,
verbose = TRUE)
preText_score_plot(preText_results)
## Introduction to Text-as-Data in R
## Date: February 6, 2025
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## Topics:
## - dfm and tf idf
## - examining your corpus
## - regular expressions
## - preText
################################################# Precept 2: Processing Text in R
## 1.1) Working Directory ------------------------------------------------------
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## 1.2) Package Management -----------------------------------------------------
## only do these once
#install.packages("pacman")
#install.packages("devtools")
#devtools::install_github("matthewjdenny/preText")
# devtools package required to install quanteda from Github
#remotes::install_github("quanteda/quanteda.textmodels")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## 1.3) Load Movie Reviews Into a Corpus ---------------------------------------
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## what constitutes a DOCUMENT in this corpus?
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove( stopwords("en") ) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
## weighted dfms
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
#######################################
## 1.4) IN CLASS ACTIVITY
## Working in pairs, compute the difference-in-means for the RATE AT WHICH
## the term "bad" occurs in positive reviews versus negative reviews
## Hint: retrieve non-zero entries for a single dfm feature column
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
pos <- dfm_subset(reviews.dfm, sentiment==1)
neg <- dfm_subset(reviews.dfm, sentiment==0)
bad_pos <- dfm_select(pos, "bad")
bad_neg <- dfm_select(neg, "bad")
bad_pos_rate <- sum(as.matrix(bad_pos)[ , 1 ]) /nrow(bad_pos)
bad_neg_rate <- sum(as.matrix(bad_neg)[ , 1 ]) /nrow(bad_neg)
bad_pos_rate - bad_neg_rate
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
head(kwic_love)
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
## all n-grams between two and four
reviews.colloc.2_4 <- textstat_collocations(reviews, size = 2:4)
reviews.colloc.2_4[1:10 , ]
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
head(preprocessed_documents$choices)
preText_results <- preText(preprocessed_documents,
dataset_name = "SOTU Speeches",
distance_method = "cosine",
num_comparisons = 20,
verbose = TRUE)
preText_score_plot(preText_results)
rm(list = ls())
setwd("/Users/christianbaehr/Documents/GitHub/POL504_precept_2023/")
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25//")
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## load packages
#devtools::install_github("quanteda/quanteda.corpora")
pacman::p_load(dplyr, ggplot2, gutenbergr, preText, quanteda, quanteda.corpora,
quanteda.textplots, quanteda.textstats, readtext)
## load movie reviews
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
reviews.tok <- tokens(reviews, remove_punct = TRUE)
Tee <- sum(lengths(reviews.tok)) # number of tokens
reviews.dfm <- dfm(reviews.tok)
M <- nfeat(reviews.dfm)  # number of types
k <- 44
b <- 0.49
k * ( Tee^b )
M
k <- 44
b <- 0.47
k * Tee^b
M
1:100
topfeatures(reviews.dfm, 100)
## plot rank (x) and frequency (y)
plot(log(1:100), log(topfeatures(reviews.dfm, 100)),
xlab = "log(rank)", ylab = "log(frequency)")
## log-log regression of frequency on rank
reg <- lm(log(topfeatures(reviews.dfm, 100)) ~ log(1:100))
summary(reg)
reviews.dfm.nostop <- reviews.tok %>%
dfm() %>%
dfm_remove(pattern=stopwords("english"))
reg.nostop <- lm(log(topfeatures(reviews.dfm.nostop, 100)) ~ log(1:100))
confint(reg.nostop)
par(mfrow = c(1, 2)) # visualize both
plot(log(1:100), log(topfeatures(reviews.dfm, 100)),
xlab = "log(rank)", ylab = "log(frequency)")
## add fitted line from regression to plot
abline(reg, col = "red")
## Zipfs prediction
abline(a = reg$coefficients[1], b = -1, col = "black")
plot(log(1:100), log(topfeatures(reviews.dfm.nostop, 100)),
xlab = "log(rank)", ylab = "log(frequency)")
abline(reg.nostop, col = "red")
abline(a = reg.nostop$coefficients[1], b = -1, col = "black")
reviews.dfm <- tokens(reviews, # tokenize
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # remove punctuation/symbols/numbers/urls
tokens_remove(stopwords("en")) |> # stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # quanteda stemmer
dfm()
## lets focus on three reviews for simplicity
indices <- c(24, 25, 48)
as.character(reviews[indices]) # take a peek
reviews.3 <- dfm_subset(reviews.dfm, subset = 1:nrow(reviews.dfm) %in% indices)
View(reviews.3)
reviews.2
reviews.3
## compute the COSINE similarity of the documents
textstat_simil(reviews.3, method = c("cosine")) # what do higher values mean?
rm(list = ls())
## load speeches from Irish parliamentary budget debates
data("data_corpus_irishbudgets")
budget.dfm <- tokens(data_corpus_irishbudgets,
remove_punct = TRUE) %>% dfm()
## we'll focus only on large parties (>1 speech)
parties <- docvars(data_corpus_irishbudgets)["party"] |>
table() # number of speeches per party
parties
largeparty <- names(parties)[parties>1] # character vector w large party names
largeparty
## keep large parties
budget.lp <- corpus_subset(data_corpus_irishbudgets, party %in% largeparty)
budget.lp <- budget.lp[budget.lp != ""] # omit empty speech
## compute FRE by document
fre <- textstat_readability(budget.lp, measure = "Flesch") |>
cbind(docvars(budget.lp)["party"])
View(fre)
## average over FRE scores, by party
fre.party <- aggregate(fre$Flesch, by=list(fre$party), mean) |>
setNames(c("party", "fre"))
View(fre)
## average over FRE scores, by party
fre.party <- aggregate(fre$Flesch, by=list(fre$party), mean) |>
setNames(c("party", "fre"))
View(fre.party)
## plot point estimates
ggplot(fre.party, aes(x = party, y = fre, colour = party)) +
geom_point() +
coord_flip() +
theme_bw() +
scale_y_continuous(breaks=seq(floor(min(fre.party$fre)),
ceiling(max(fre.party$fre)),
by = 2)) +
xlab("") + ylab("Flesch Score Point Estimates, by Party") +
theme(legend.position = "none")
## lets first break up texts by party
budget.lp.df <- data.frame(budget.lp) |>
cbind(docvars(budget.lp)["party"]) |>
setNames(c("text", "party"))
budget.lp.df.SPLIT <- split(budget.lp.df, f=as.factor(budget.lp.df$party))
View(budget.lp.df.SPLIT)
boot.fre <- function(party) { # accepts df of texts (party-specific)
n <- nrow(party) # number of texts
docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
docs.boot <- party[docnums, "text"]
docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
return(mean(fre[,"Flesch"])) # return flesch scores only
}
## how does it look?
lapply(budget.lp.df.SPLIT, boot.fre) # apply to each df of party texts
fre.party
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
1:iter
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(budget.lp.df.SPLIT, boot.fre)
print(i)
}
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(largeparty, mean.boot, sd.boot) |>
setNames(c("party", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = party)) + # general setup for plot
geom_linerange(aes(x = party,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = party,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Party") + # fancy stuff
theme(legend.position = "none") # fancy stuff
rm(lis)
rm(list = ls())

}
lapply(unga.split, boot.fre) # apply to each df of party texts
iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)
## for loop to compute as many samples as specified
for(i in 1:iter) {
if(i==1) {boot.means <- list()} # generate new list
# store the results in new element i
boot.means[[i]] <- lapply(unga.split, boot.fre)
print(paste("Iteration", i))
}
## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)
## create data frame for plot
plot_df <- data.frame(sort(unique(unga.df$year)), mean.boot, sd.boot) |>
setNames(c("year", "mean", "se"))
## confidence intervals
ci90 <- qnorm(0.95)
ci95 <- qnorm(0.975)
## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
geom_linerange(aes(x = year,
ymin = mean - se*ci90,
ymax = mean + se*ci90),
lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
geom_pointrange(aes(x = year,
y = mean,
ymin = mean - se*ci95,
ymax = mean + se*ci95),
lwd = 1/2, position = position_dodge(width = 1/2),
shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
#coord_flip() + # fancy stuff
theme_bw() + # fancy stuff
xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
theme(legend.position = "none") # fancy stuff
## mean Flesch statistic by year
flesch_point <- unga.df$text %>%
textstat_readability(measure = "Flesch") %>%
group_by(unga.df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>%
arrange(as.numeric(year))
cbind(flesch_point, "bs_mean" = plot_df$mean)
## calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(unga.sub, c("Flesch", "FOG"))
## compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))
## print
print(readability_cor[1,2])
docs <- corpus( readtext(c("hw1/jefferson_draft.txt", "hw1/final_version.txt")))
# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words", gap=-1)
# increase match, decrease mismatch --> increases extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2],
type="words",
match= 3,
mismatch = 0,
gap=-1)
print(sw2$sw)
print(sw3$sw)
rc <- read.csv("hw1/countypres_2000-2020.csv")
rc20 <- rc[rc$year=="2020",]
rc20DR <- rc20[rc20$party%in%c("REPUBLICAN", "DEMOCRAT"),]
# look at specific state
# TX works
state <- "VT"
rc_state <- rc20DR[rc20DR$state_po==state,]
#look at leading digit v frequency
digits <- rc_state$candidatevotes
first_digits <-  as.numeric(substr(digits, 1, 1))
rc_state$first_digs <- first_digits
dems <- rc_state[rc_state$party=="DEMOCRAT",]
total_dems <- table(factor(dems$first_digs, levels=1:9) )
rep <-  rc_state[rc_state$party=="REPUBLICAN",]
total_reps <- table(factor(rep$first_digs, levels=1:9) )
#benford expectations
ben_props  <- c(0.301, .176, .125, .097, 0.079, 0.067, 0.058, 0.051, 0.046)
names(ben_props)<- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
the_benprops <- ben_props*length(unique(rc_state$county_name))
# now do barplot
dat <- rbind(total_dems, the_benprops, total_reps)
rownames(dat) <- c("dem", "benford", "rep")
colnames(dat) <- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
#x11()
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
# article:
# https://www.reuters.com/article/world/fact-check-deviation-from-benfords-law-does-not-prove-election-fraud-idUSKBN27Q3A9/
# Election Integrity Partnership
# look at magnitude of districts
x11()
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not
state <- "TX"
rc_state <- rc20DR[rc20DR$state_po==state,]
#look at leading digit v frequency
digits <- rc_state$candidatevotes
first_digits <-  as.numeric(substr(digits, 1, 1))
rc_state$first_digs <- first_digits
dems <- rc_state[rc_state$party=="DEMOCRAT",]
total_dems <- table(factor(dems$first_digs, levels=1:9) )
rep <-  rc_state[rc_state$party=="REPUBLICAN",]
total_reps <- table(factor(rep$first_digs, levels=1:9) )
#benford expectations
ben_props  <- c(0.301, .176, .125, .097, 0.079, 0.067, 0.058, 0.051, 0.046)
names(ben_props)<- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
the_benprops <- ben_props*length(unique(rc_state$county_name))
# now do barplot
dat <- rbind(total_dems, the_benprops, total_reps)
rownames(dat) <- c("dem", "benford", "rep")
colnames(dat) <- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")
#x11()
barplot(dat, col = c("blue","black","red"), beside = T, main=state)
# install.packages("devtools")
devtools::install_github("oscarkjell/text")
dyn.load(jli, FALSE)
install.packages("rJava", repos="https://rforge.net")
install.packages("rJava")
# install.packages("devtools")
devtools::install_github("oscarkjell/text")
library(rJava)
install.packages("rJava")
library("rJava")
pak::pak("ropensci/gutenbergr")
install.packages("pak")
pak::pak("ropensci/gutenbergr")
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25//")
## load packages
#install.packages("remotes")
#remotes::install_github("kbenoit/quanteda.dictionaries")
#devtools::install_github("oscarkjell/text")
pacman::p_load(dplyr, gutenbergr, quanteda, quanteda.corpora,
quanteda.dictionaries, readtext, stringr, stylest2,
SentimentAnalysis, sentimentr)
## what do they have by Jane?
books <- gutenberg_works() |>
filter(author=="Melville, Herman")
## download just Pride and Prejudice
moby <- gutenberg_download(gutenberg_id = 2489)
View(moby)
help("gutenberg_download")
## download just Pride and Prejudice
moby <- gutenberg_download(gutenberg_id = 2489,
mirror = "https://gutenberg.nabasny.com/")
## download just Pride and Prejudice
moby <- gutenberg_download(gutenberg_id = 2489,
mirror = "https://aleph.gutenberg.org/")
## download just Pride and Prejudice
moby <- gutenberg_download(gutenberg_id = 2489,
mirror = "https://aleph.gutenberg.org")
## what do they have by Jane?
books <- gutenberg_works() |>
filter(author=="Austen, Jane")
View(books)
## download just Pride and Prejudice
pride <- gutenberg_download(gutenberg_id = 1342,
mirror = "https://aleph.gutenberg.org")
## download just Pride and Prejudice
pride <- gutenberg_download(gutenberg_id = "1342-0",
mirror = "https://aleph.gutenberg.org")
## download just Pride and Prejudice
pride <- gutenberg_download(gutenberg_id = "1342",
mirror = "https://aleph.gutenberg.org")
## download just Pride and Prejudice
df <- gutenberg_download(1184)
View(df)
## what do they have by Jane?
books <- gutenberg_works() |>
filter(gutenberg_id==1184)
View(books)
## what do they have by Jane?
books <- gutenberg_works() |>
filter(author=="Dumas, Alexandre")
pride <- gutenberg_download(gutenberg_id = "1184")
View(df)
count <- gutenberg_download(gutenberg_id = 1184)
View(count)
rm(list = ls())
## the stylest2 package comes with some example data
data("novels")
View(novels)
## who are the authors?
unique(novels$author)
novels_dfm <- novels |>
corpus(text_field = "text") |>
tokens() |>
dfm()
## cross validation
set.seed(123)
s2_cv <- stylest2_select_vocab(dfm = novels_dfm,
smoothing = 0.5,
nfold = 5,
cutoffs = seq(10, 90, 10))
help(corpus)
s2_cv$cutoff_pct_best # cutoff percentile with best performance
s2_cv$cv_missrate_results
apply(s2_cv$cv_missrate_results, 2, mean) # average miss pct. by percentile
dfm=novels_dfm
nfold=5
## reorder documents prior distributing folds
ndoc <- nrow(dfm)
## assign rows to folds
test_fold <- sample(rep(1:nfold, ceiling(ndoc / nfold)), ndoc)
ndoc
s2_cv$cutoff_pct_best # cutoff percentile with best performance
s2_cv$cv_missrate_results
apply(s2_cv$cv_missrate_results, 2, mean) # average miss pct. by percentile
s2_terms <- stylest2_terms(novels_dfm,
cutoff = s2_cv$cutoff_pct_best)
s2_fit <- stylest2_fit(novels_dfm,
terms = s2_terms)
## explore output
View(s2_fit)
## most frequently used terms by each author
term_usage <- s2_fit$rate
View(term_usage)
authors <- rownames(term_usage)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
s2_pred <- stylest2_predict(novels_dfm, s2_fit, term_influence = T)
s2_pred$term_influence
## most influential terms overall
head(stylest_term_influence(mod.fit,
novels_excerpts$text,
novels_excerpts$author))
gutenbergr::gutenberg_works(author=="Austen, Jane")
## process a Pride and Prejudice excerpt to predict author
gutenbergr::gutenberg_download(1342)
## process a Pride and Prejudice excerpt to predict author
gutenbergr::gutenberg_download(gutenberg_id==1342)
help(gutenberg_download)
gutenbergr::gutenberg_download(gutenberg_id=1342)
## process a Pride and Prejudice excerpt to predict author
pride <- gutenbergr::gutenberg_download(gutenberg_id=1342)
View(pride)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] %>% # randomly select 100
paste(collapse = " ") # collapse to single string
pred.text
pride$text[1]
pride$text[50]
pride$text[5000]
## fit the model using the Pride excerpt
pred <- stylest_predict(mod.fit, pred.text)
## fit the model using the Pride excerpt
pred <- stylest2_predict(mod.fit, pred.text)
## fit the model using the Pride excerpt
pred <- stylest2_predict(s2_fit, pred.text)
pred <- stylest2_predict(s2_fit, pred.text)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
## fit the model using the Pride excerpt
pred <- stylest2_predict(s2_fit, pred.text)
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(s2_fit, pred.text)
class(pred.text)
help(stylest2_fit)
help(stylest2_predict)
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
View(pred)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$predicted # predicted author
pred$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$predicted # predicted author
pred$log_probs # log probabilities of authorship
View(pred)
pred$posterior$predicted
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs
pred$posterior$predicted # predicted author
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
#docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
pride <- gutenbergr::gutenberg_download(gutenberg_id=1342)
setwd(123)
## Introduction to R Programming
## Date: February 20, 2025
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## Topics:
## - speaker and style distinctiveness
## - regular expressions pt. 2
## - dictionaries
################################################# Precept 4: Supervised Text Analysis I
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25//")
## load packages
#install.packages("remotes")
#remotes::install_github("kbenoit/quanteda.dictionaries")
#devtools::install_github("oscarkjell/text")
pacman::p_load(dplyr, quanteda, quanteda.corpora,
quanteda.dictionaries, readtext, stringr, stylest2,
SentimentAnalysis, sentimentr)
## 1.1) Predicting authorship using stylest2 -----------------------------------
## we want to build a model to can predict who authored a text
## the stylest2 package comes with some example data
data("novels")
## who are the authors?
unique(novels$author)
## step 1 is identify which features are strong out-of-sample predictors of
## authorship
novels_dfm <- novels |>
corpus(text_field = "text") |>
tokens() |>
dfm()
## we use k-fold cross validation to determine these features and prevent overfitting
## cross validation
set.seed(123)
s2_cv <- stylest2_select_vocab(dfm = novels_dfm,
smoothing = 0.5,
nfold = 5,
cutoffs = seq(10, 90, 10))
s2_cv$cutoff_pct_best # cutoff percentile with best performance
s2_cv$cv_missrate_results
apply(s2_cv$cv_missrate_results, 2, mean) # average miss pct. by percentile
## now pick the subset of features that fall above the optimal cutoff. We will estimate
## the final model over the full data using only these features.
s2_terms <- stylest2_terms(novels_dfm,
cutoff = s2_cv$cutoff_pct_best)
## estimate the model of speaker identity based on the selected features
s2_fit <- stylest2_fit(novels_dfm,
terms = s2_terms)
## explore output
View(s2_fit)
## most frequently used terms by each author
term_usage <- s2_fit$rate
authors <- rownames(term_usage)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
s2_pred <- stylest2_predict(novels_dfm, s2_fit, term_influence = T)
s2_pred$term_influence
## process a Pride and Prejudice excerpt to predict author
pride <- gutenbergr::gutenberg_download(gutenberg_id=1342)
set.seed(123)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
#docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship
## Introduction to R Programming
## Date: February 20, 2025
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## Topics:
## - speaker and style distinctiveness
## - regular expressions pt. 2
## - dictionaries
################################################# Precept 4: Supervised Text Analysis I
## working directory
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25//")
## load packages
#install.packages("remotes")
#remotes::install_github("kbenoit/quanteda.dictionaries")
#devtools::install_github("oscarkjell/text")
pacman::p_load(dplyr, quanteda, quanteda.corpora,
quanteda.dictionaries, readtext, stringr, stylest2,
SentimentAnalysis, sentimentr)
## 1.1) Predicting authorship using stylest2 -----------------------------------
## we want to build a model to can predict who authored a text
## the stylest2 package comes with some example data
data("novels")
## who are the authors?
unique(novels$author)
## step 1 is identify which features are strong out-of-sample predictors of
## authorship
novels_dfm <- novels |>
corpus(text_field = "text") |>
tokens() |>
dfm()
## we use k-fold cross validation to determine these features and prevent overfitting
## cross validation
set.seed(123)
s2_cv <- stylest2_select_vocab(dfm = novels_dfm,
smoothing = 0.5,
nfold = 5,
cutoffs = seq(10, 90, 10))
s2_cv$cutoff_pct_best # cutoff percentile with best performance
s2_cv$cv_missrate_results
apply(s2_cv$cv_missrate_results, 2, mean) # average miss pct. by percentile
## now pick the subset of features that fall above the optimal cutoff. We will estimate
## the final model over the full data using only these features.
s2_terms <- stylest2_terms(novels_dfm,
cutoff = s2_cv$cutoff_pct_best)
## estimate the model of speaker identity based on the selected features
s2_fit <- stylest2_fit(novels_dfm,
terms = s2_terms)
## explore output
View(s2_fit)
## most frequently used terms by each author
term_usage <- s2_fit$rate
authors <- rownames(term_usage)
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) |>
setNames(authors)
s2_pred <- stylest2_predict(novels_dfm, s2_fit, term_influence = T)
s2_pred$term_influence
## process a Pride and Prejudice excerpt to predict author
pride <- gutenbergr::gutenberg_download(gutenberg_id=1342)
set.seed(456)
pred.text <- pride$text[pride$text!=""] %>% # omit empty excepts
.[sample(1:length(.), 1000)] |> # randomly select 100
paste(collapse = " ") |> # collapse to single string
tokens() |>
dfm()
docvars(pred.text)["author"] <- "Austen, Jane"
## fit the model using the Pride excerpt
pred <- stylest2_predict(pred.text, s2_fit)
pred$posterior$predicted # predicted author
pred$posterior$log_probs # log probabilities of authorship

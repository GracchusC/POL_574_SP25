help(remove.packages)
remove.packages("fect")
devtools::install_github("xuyiqing/fect")
library(devtools)
help(install_github)
devtools::install_github("xuyiqing/fect", upgrade="always")
devtools::install_github("xuyiqing/fect", upgrade="always")
rm(list=ls())
# load packages
pacman::p_load(tidyverse, fixest, modelsummary, kableExtra, corrplot)
# set working directory
if(Sys.info()["user"]=="fiona" ) {setwd("/Users/fiona/Dropbox/BBH/BBH1/")}
if(Sys.info()["user"]=="christianbaehr" ) {setwd("/Users/christianbaehr/Dropbox/BBH/BBH1/")}
if(Sys.info()["user"]=="vincentheddesheimer" ) {setwd("~/Dropbox (Princeton)/BBH/BBH1/")}
# load data
df <- read_rds("data/03_final/lobbying_df_quarterly_REVISE_normal_NEW.rds")
df_old <- read_rds("data/03_final/lobbying_df_quarterly_REVISE_normal.rds")
names(df)
names(df_old)
compare <- function(varname) {
m1 <- round(mean(df[which(df$year<2020) , varname], na.rm=T), 5)
m2 <- round(mean(df_old[which(df_old$year<2020) , varname], na.rm=T), 5)
s1 <- round(sd(df[which(df$year<2020) , varname], na.rm=T), 5)
s2 <- round(sd(df_old[which(df_old$year<2020) , varname], na.rm=T), 5)
return(cat(i, "\n", sprintf("Old df: mean = %s SD = %s \nNew df: mean = %s SD = %s", m2, s2, m1, s1), "\n"))
}
for(i in grep("CLI_", names(df), value=T)) {print(compare(i))}
## Significant diffs for EPA
## Add SPO to the data
## Significant diffs for NOAA
## Significant diffs for a few others, but not to the degree of implausibility
# Rename fixed effects variables
df <- df |>
mutate(
Firm = isin,
Year = year,
Industry = industry,
`Industry x Year` = industry_year
)
# Specify covariate names
cm <- c("op_expo_ew" = "Opportunity Exposure",
"rg_expo_ew" = "Regulatory Exposure",
"ph_expo_ew" = "Physical Exposure",
"op_sent_ew" = "Opportunity Sentiment",
"rg_sent_ew" = "Regulatory Sentiment",
"ph_sent_ew" = "Physical Sentiment",
"op_pos_ew" = "Opportunity Pos. Sent.",
"rg_pos_ew" = "Regulatory Pos. Sent.",
"ph_pos_ew" = "Physical Pos. Sent.",
"op_neg_ew" = "Opportunity Neg. Sent.",
"rg_neg_ew" = "Regulatory Neg. Sent.",
"ph_neg_ew" = "Physical Neg. Sent.",
"op_risk_ew" = "Opportunity Risk",
"rg_risk_ew" = "Regulatory Risk",
"ph_risk_ew" = "Physical Risk",
"cc_expo_ew" = "Overall Exposure",
"op_expo_ew:rg_expo_ew" = "Opp. x Reg.",
"op_expo_ew:ph_expo_ew" = "Opp. x Phy.",
"rg_expo_ew:ph_expo_ew" = "Reg. x Phy.",
"op_expo_ew:op_sent_ew" = "Opp. x Sent.",
"rg_expo_ew:rg_sent_ew" = "Reg. x Sent.",
"ph_expo_ew:ph_sent_ew" = "Phy. x Sent.",
"op_expo_ew:cc_sent_ew" = "Opp. x General Sent.",
"cc_sent_ew:rg_expo_ew" = "Reg. x General Sent.",
"cc_sent_ew:ph_expo_ew" = "Phy. x General Sent.",
"op_expo_ew:op_pos_ew" = "Opp. x Pos.",
"rg_expo_ew:rg_pos_ew" = "Reg. x Pos.",
"ph_expo_ew:ph_pos_ew" = "Phy. x Pos.",
"op_expo_ew:op_neg_ew" = "Opp. x Neg.",
"rg_expo_ew:rg_neg_ew" = "Reg. x Neg.",
"ph_expo_ew:ph_neg_ew" = "Phy. x Neg.",
"ebit" = "EBIT",
"ebit_at" = "EBIT/Assets",
"us_dummy" = "US HQ",
"total_lobby_quarter" = "Total Lobbying (\\$)",
"CLI_l1" = "Lagged DV",
"log_CLI_amount_l1" = "Lagged DV"
)
df$CLI <- as.numeric(df$CLI_quarter)
df$log_CLI_amount <- log(df$CLI_amount_quarter + 1)
df <- df %>%
group_by(Firm) %>%
mutate(CLI_l1 = lag(CLI, n=1, order_by=yearqtr),
log_CLI_amount_l1 = lag(log_CLI_amount, n=1, order_by=yearqtr),
op_expo_ew_l1 = lag(op_expo_ew, n=1, order_by=yearqtr),
rg_expo_ew_l1 = lag(rg_expo_ew, n=1, order_by=yearqtr),
ph_expo_ew_l1 = lag(ph_expo_ew, n=1, order_by=yearqtr))
df$CLI_chg <- df$CLI - df$CLI_l1
df$log_CLI_amount_chg <- df$log_CLI_amount - df$log_CLI_amount_l1
df$op_expo_ew_chg <- df$op_expo_ew - df$op_expo_ew_l1
df$rg_expo_ew_chg <- df$rg_expo_ew - df$rg_expo_ew_l1
df$ph_expo_ew_chg <- df$ph_expo_ew - df$ph_expo_ew_l1
glimpse(df)
feols(CLI ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df, vcov = ~ Year + Firm)
feols(CLI ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df, vcov = ~ Firm)
feols(CLI ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2021) , ], vcov = ~ Year + Firm)
feols(CLI_amount ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2021) , ], vcov = ~ Year + Firm)
names(df)
feols(log_CLI_amount ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2021) , ], vcov = ~ Year + Firm)
feols(CLI ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2020) , ], vcov = ~ Year + Firm)
feols(log_CLI_amount ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2020) , ], vcov = ~ Year + Firm)
2.5828e-01
feols(log_CLI_amount ~ op_expo_ew + rg_expo_ew + ph_expo_ew + ebit + ebit_at + us_dummy + total_lobby_quarter + CLI_l1 | `Industry x Year`, data=df[which(df$year<2020) , ], vcov = ~ Firm)
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
devtools::install_github("matthewjdenny/preText")
install.packages("quanteda.textmodels")
devtools::install_github("quanteda/quanteda.textmodels")
devtools::install_github("matthewjdenny/preText")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove(stopwords("en")) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
## Hint: retrieve non-zero entries for a single dfm feature column
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
pos <- dfm_subset(reviews.dfm, subset = sentiment==1)
neg <- dfm_subset(reviews.dfm, subset = sentiment==0)
pos.col <- dfm_select(pos, pattern = "bad")
neg.col <- dfm_select(neg, pattern = "bad")
sum(pos.col) / dim(pos.col)[1] - sum(neg.col) / dim(neg.col)[1]
## Hint: retrieve non-zero entries for a single dfm feature column
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
## 2.1) key word in context
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
## 2.2) Collocations
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
## how to interpret?
## trigrams
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
## all n-grams between two and four
reviews.colloc.2_4 <- textstat_collocations(reviews, size = 2:4)
## it would take an entire course to teach you (or me) all there is know about
## regular expressions
## - RegEx cheatsheet in Github
## - test your regular expression: https://regex101.com/
## break this sentence a character vector of words
mysentence <- "We've never lost an American in space, we're sure as hell not gonna lose one on my watch! Failure is not an option."
mysentence <- strsplit(mysentence, split = " ")[[1]]
grep("a", mysentence, value = T) # any element with "a"
grep("^a", mysentence, value = T) # any element STARTING with "a"
grep("[[:punct:]]", mysentence, value = T) # any element with punctuation
grep("^F+[a-z0-9]+ure$", mysentence, value = T)
## starts with F, contains middle characters with a-z and/or 0-9 and ends with "ure"
grep("^F{1}[a-z0-9]{3}ure$", mysentence, value = T) # why does this work?
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
head(preprocessed_documents$choices)
preText_results <- preText(preprocessed_documents,
dataset_name = "SOTU Speeches",
distance_method = "cosine",
num_comparisons = 20,
verbose = TRUE)
preText_score_plot(preText_results)
mysentence <- "If we played 'em ten times, they might win nine. But not this game. Not...tonight."
tolower(mysentence) # lowercase
toupper(mysentence) # uppercase
gsub("not", "NOT", mysentence) # substitute "not" with "NOT"
gsub(",", "", mysentence) # works for punctuation
gsub(".", "", mysentence) # some characters are "special"
gsub("\\.", "", mysentence) # escape with double backslash
## define a function to strip periods
remove.periods <- function(text) {
out <- gsub("\\.", " ", text) # note the escape character "\\" for period
return(out) # return() to retrieve output
}
remove.periods(mysentence)
remove.punctuation <- function(text) {
out <- gsub("\\.|,|'", " ", text) # drop punctuation (why "|"?)
return(out)
}
remove.punctuation(mysentence)
process.text <- function(text) {
out <- gsub("\\.|,|'", " ", text) # sub. punctuation
out.lower <- tolower(out) # all lowercase
out.lower.singlespace <- gsub("\\s+", " ", out.lower) # remove redundant spaces
return(out.lower.singlespace)
}
mycleansentence <- process.text(mysentence) # notice no function objects are global
mycleanwords <- strsplit(mycleansentence, split = " ")[[1]] # strsplit() breaks on split
grep("not", mycleanwords) # indices that CONTAIN not
grep("play", mycleanwords) # indices that CONTAIN play
grep("play", mycleanwords, value = T) # return entire matching word/s
grepl("play", mycleanwords) # boolean
## identify which elements contain a specified string
grep("not", mycleanwords) # indices that CONTAIN not
grep("play", mycleanwords) # indices that CONTAIN play
grep("play", mycleanwords, value = T) # return entire matching word/s
grepl("play", mycleanwords) # boolean
library(quanteda)
library(readtext)
## 4.2) load text data into R using readtext() and corpus(). You can also
## pass R data frames to corpus().
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
reviews.raw <- readtext("data/reviews.csv")
reviews <- corpus(reviews.raw$review) # make text variable into a "corpus"
## we can easily add metadata
docvars(reviews, "text") <- reviews.raw$text
docvars(reviews, "sentiment") <- reviews.raw$sentiment
summary(reviews, 10) # retrieve document level info
## SHORTCUT: add "text_field" to readtext()
reviews.raw <- readtext("data/reviews.csv", text_field = "review")
reviews <- corpus(reviews.raw)
reviews.pos <- corpus_subset(reviews, sentiment==1)
reviews.pos <- corpus_subset(reviews, sentiment==1)
reviews.pos.tokens <- tokens(reviews.pos) # no pre processing
reviews.pos.tokens[1]
reviews.pos[1]
reviews.pos.dfm <- dfm(reviews.pos.tokens)
reviews.pos.dfm[1,]
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en")) |>
tokens_remove("br")
tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleantokens
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en"))
reviews.pos.cleandfm <- dfm(reviews.pos.cleantokens)
dim(reviews.pos.dfm)
dim(reviews.pos.cleandfm) # fewer features
(2840-2633) / 2840 # pre processing dropped ~7% of features from unprocessed
## pull the top n frequently appearing features from the document feature matrix
topfeatures(reviews.pos.cleandfm, n=20)
## wordcloud visual of top features
library(quanteda.textplots)
textplot_wordcloud(reviews.pos.cleandfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.pos.cleantokens <- tokens(reviews.pos,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T,
remove_separators = T) |>
tokens_remove(stopwords("en")) |>
tokens_remove("br")
reviews.pos.cleandfm <- dfm(reviews.pos.cleantokens)
dim(reviews.pos.dfm)
dim(reviews.pos.cleandfm) # fewer features
(2840-2633) / 2840 # pre processing dropped ~7% of features from unprocessed
## 4.4) we can sanity check our document feature matrix
## pull the top n frequently appearing features from the document feature matrix
topfeatures(reviews.pos.cleandfm, n=20)
## wordcloud visual of top features
library(quanteda.textplots)
textplot_wordcloud(reviews.pos.cleandfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/") # change the working directory
setwd("../") # move one directory back
ls() # objects in the global environment
mysentence <- "If we played 'em ten times, they might win nine. But not this game. Not...tonight."
tolower(mysentence) # lowercase
toupper(mysentence) # uppercase
gsub("not", "NOT", mysentence) # substitute "not" with "NOT"
gsub(",", "", mysentence) # works for punctuation
gsub("not", "NOT", mysentence) # substitute "not" with "NOT"
gsub(",", "", mysentence) # works for punctuation
gsub(".", "", mysentence) # some characters are "special"
gsub("\\.", "", mysentence) # escape with double backslash
gsub(".", "", mysentence) # some characters are "special"
remove.periods <- function(text) {
out <- gsub("\\.", " ", text) # note the escape character "\\" for period
return(out) # return() to retrieve output
}
remove.periods(mysentence)
mycleanwords <- strsplit(mycleansentence, split = " ")[[1]] # strsplit() breaks on split
## identify which elements contain a specified string
grep("not", mycleanwords) # indices that CONTAIN not
grep("play", mycleanwords) # indices that CONTAIN play
grep("play", mycleanwords, value = T) # return entire matching word/s
grepl("play", mycleanwords) # boolean
grep("not", mycleanwords)
grep("play", mycleanwords)
grep("play", mycleanwords, value = T)
grepl("play", mycleanwords)
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
summary(reviews, n=ndoc(reviews))
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
corpus_reshape(reviews, to = "sentences")
tokens_wordstem()
stopwords("en")
"br"
reviews
reviews[1]
reviews[[1]]
help("stopwords") # quanteda has stopwords for MANY languages
help("quanteda::stopwords") # quanteda has stopwords for MANY languages
stopwords("aq")
stopwords("sq")
stopwords("da")
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
help(dfm_tfidf)
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # dfm() defaults to "absolute frequency"
help(dfm_tfidf)
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
help(kwic)
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
## how to interpret?
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc.3
## Introduction to Text-as-Data in R
## Date: February 6, 2025
## Author: Christian Baehr
## Lab adapted from: Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin
## Munger, Patrick Chester and Leslie Huang.
## Topics:
## - dfm and tf idf
## - examining your corpus
## - regular expressions
## - preText
################################################# Precept 2: Processing Text in R
## 1.1) Working Directory ------------------------------------------------------
## point this to directory containing precept files
setwd("/Users/christianbaehr/Documents/GitHub/POL_574_SP25/")
## 1.2) Package Management -----------------------------------------------------
## only do these once
#install.packages("pacman")
#install.packages("devtools")
#devtools::install_github("matthewjdenny/preText")
## use pacman instead of loading individual libraries
pacman::p_load(ggplot2,
preText,
quanteda,
quanteda.textplots,
quanteda.textstats,
readtext)
## 1.3) Load Movie Reviews Into a Corpus ---------------------------------------
## load csv with text in "review" column
reviews <- readtext("data/reviews.csv", text_field = "review") |>
corpus()
## what constitutes a DOCUMENT in this corpus?
## retrieve document level info
reviews.info <- summary(reviews, n=ndoc(reviews)) # document info
sentence_plot <- ggplot(data = reviews.info, aes(x = Sentences)) +
geom_histogram() # distribution of the number of sentences
sentence_plot
## (we could convert to sentence level corpus)
corpus_reshape(reviews, to = "sentences")
## lets build a document feature matrix
reviews.dfm <- tokens(reviews,
remove_punct = T,
remove_symbols = T,
remove_numbers = T,
remove_url = T) |> # tokenize, remove punctuation/symbols/numbers/urls
tokens_remove( stopwords("en") ) |> # remove stopwords
tokens_remove("br") |> # remove "br" (Gucci Mane is not a movie reviewer)
tokens_wordstem() |> # use the quanteda stemmer
dfm()
help("stopwords") # quanteda has stopwords for MANY languages
topfeatures(reviews.dfm)
textplot_wordcloud(reviews.dfm, min_count = 5, random_order = F, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
## weighted dfms
reviews.dfm_weighted <- dfm_tfidf(reviews.dfm, base = exp(1)) # defaults to "absolute frequency"
## ALSO DEFAULTS TO LOG_10 !!!
reviews.dfm_prop <- dfm_tfidf(reviews.dfm,
scheme_tf = "prop",
base = exp(1)) # relative frequency
topfeatures(reviews.dfm_prop)
topfeatures(reviews.dfm_prop[nrow(reviews.dfm_prop),])
topfeatures(reviews.dfm_weighted)
topfeatures(reviews.dfm_weighted[nrow(reviews.dfm_weighted),]) # why is ordering identical as frequency weighting?
#######################################
## 1.4) IN CLASS ACTIVITY
## Working in pairs, compute the difference-in-means for the RATE AT WHICH
## the term "bad" occurs in positive reviews versus negative reviews
## Hint: retrieve non-zero entries for a single dfm feature column
feature <- dfm_select(reviews.dfm, pattern = "love") # subset dfm to single feature
occurrences <- feature@x # extract the non-zero rows for "love"
#######################################
## 2.1) Key Word in Context ----------------------------------------------------
kwic_love <- kwic(tokens(reviews),
pattern = "love", # occurrences of "love"
valuetype = "fixed",
window = 4) # four words on either side
head(kwic_love)
head(kwic(tokens(reviews), pattern = "hate")) # what is the problem here
## 2.2) Collocations -----------------------------------------------------------
reviews.colloc <- textstat_collocations(reviews) # default is bigrams
## how to interpret?
## trigrams
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc
reviews.colloc.3 <- textstat_collocations(reviews, size = 3)
reviews.colloc.3
## all n-grams between two and four
reviews.colloc.2_4 <- textstat_collocations(reviews, size = 2:4)
reviews.colloc.2_4
## break this sentence a character vector of words
mysentence <- "We've never lost an American in space, we're sure as hell not gonna lose one on my watch! Failure is not an option."
mysentence <- strsplit(mysentence, split = " ")[[1]]
grep("a", mysentence, value = T) # any element with "a"
grep("^a", mysentence, value = T) # any element STARTING with "a"
grep("[[:punct:]]", mysentence, value = T) # any element with punctuation
grep("^F+[a-z0-9]+ure$", mysentence, value = T)
grep("^F{1}[a-z0-9]{3}ure$", mysentence, value = T) # why does this work?
preprocessed_documents <- factorial_preprocessing(
reviews,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2,
verbose = TRUE)
head(preprocessed_documents$choices)
preText_results <- preText(preprocessed_documents,
dataset_name = "SOTU Speeches",
distance_method = "cosine",
num_comparisons = 20,
verbose = TRUE)
preText_score_plot(preText_results)
